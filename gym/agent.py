import collections as defaultdict
import gymnasium as gym
import numpy as np
import random

class CatanAgent:
    def __init__(
        self,
        env: gym.Env,
        learning_rate: float,
        initial_epsilon: float,
        epsilon_decay: float,
        final_epsilon: float,
        discount_factor: float = 0.95
    ):
        """Initialize a Q-Learning agent.
        Args:
            env: The training environment
            learning_rate: How quickly to update Q-values (0-1)
            initial_epsilon: Starting exploration rate (usually 1.0)
            epsilon_decay: How much to reduce epsilon each episode
            final_epsilon: Minimum exploration rate (usually 0.1)
            discount_factor: How much to value future rewards (0-1)
        """

        self.env = env

        # Q-table: maps (state, action) to expected reward
        # defaultdict automatically creates entries with zeros for new states
        ### MUST BE CHANGED
        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))

        self.learning_rate = learning_rate
        self.initial_epsilon = initial_epsilon
        self.epsilon_decay = epsilon_decay
        self.final_epsilon = final_epsilon
        self.discount_factor = discount_factor

        self.training_error = []

        def get_action(self, obs: dict) -> int:
            """Choose an action using epsilon-greedy strategy.

            Returns:
                action: 0 (stand) or 1 (hit)
            """

            legal_actions = len(obs['actions'])
            # With probability epsilon: explore (random action)
            if np.random.random() < self.epsilon:
                return random.choice(legal_actions)

            # With probability (1-epsilon): exploit (best known action)
            else:
                return int(np.argmax(self.q_values[obs]))
            
        
        def update(
            self,
            obs: dict,
            action: int,
            reward: float,
            terminated: bool,
            next_obs: dict,
        ):
            """Update Q-value based on experience.

            This is the heart of Q-learning: learn from (state, action, reward, next_state)
            """
            # What's the best we could do from the next state?
            # (Zero if episode terminated - no future rewards possible)
            future_q_value = (not terminated) * np.max(self.q_values[next_obs])

            # What should the Q-value be? (Bellman equation)
            target = reward + self.discount_factor * future_q_value

            # How wrong was our current estimate?
            temporal_difference = target - self.q_values[obs][action]

            # Update our estimate in the direction of the error
            # Learning rate controls how big steps we take
            self.q_values[obs][action] = (
                self.q_values[obs][action] + self.lr * temporal_difference
            )

            # Track learning progress (useful for debugging)
            self.training_error.append(temporal_difference)

    def decay_epsilon(self):
        """Reduce exploration rate after each episode."""
        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)